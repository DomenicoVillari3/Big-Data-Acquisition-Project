{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75052fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import io\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import boto3\n",
    "from botocore.config import Config \n",
    "from typing import Tuple,List\n",
    "import sys\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb56edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_IP = \"192.168.128.236\"  # IP del Server Master\n",
    "MINIO_BUCKET_NAME=\"satellite-data\"\n",
    "MINIO_ADDRESS=\"http://192.168.128.236:9000\"\n",
    "# Configurazione MinIO\n",
    "MINIO_ENDPOINT = f\"http://{MASTER_IP}:9000\"\n",
    "MINIO_ACCESS_KEY = \"minioadmin\"\n",
    "MINIO_SECRET_KEY = \"minioadmin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4209d4",
   "metadata": {},
   "source": [
    "## Setup Spark context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09819e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ Il mio IP Client (Driver) Ã¨: 192.168.128.236\n",
      "ðŸŽ¯ Mi connetto al Master su: 192.168.128.236\n",
      "ðŸš€ Tentativo di connessione al Cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/27 10:02:54 WARN Utils: Your hostname, PC7 resolves to a loopback address: 127.0.1.1; using 192.168.128.236 instead (on interface eno1)\n",
      "25/12/27 10:02:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/27 10:02:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/27 10:02:55 WARN Utils: Service 'sparkDriver' could not bind on port 20000. Attempting port 20001.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'sparkDriver' could not bind on port 20001. Attempting port 20002.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'sparkDriver' could not bind on port 20002. Attempting port 20003.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'sparkDriver' could not bind on port 20003. Attempting port 20004.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'sparkDriver' could not bind on port 20004. Attempting port 20005.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'sparkDriver' could not bind on port 20005. Attempting port 20006.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 20001. Attempting port 20002.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 20002. Attempting port 20003.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 20003. Attempting port 20004.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 20004. Attempting port 20005.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 20005. Attempting port 20006.\n",
      "25/12/27 10:02:55 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 20006. Attempting port 20007.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… SUCCESS! Connesso al Cluster.\n",
      "ðŸ”— Master: spark://192.168.128.236:7077\n",
      " Web UI (Locale): http://192.168.128.236:4040\n",
      " Web UI (Master): http://192.168.128.236:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Test di calcolo distribuito: 100 righe contate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Trova automaticamente l'IP della TUA macchina (Client PC7)\n",
    "# che Ã¨ visibile al Master.\n",
    "def get_local_ip(target_ip):\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:\n",
    "        try:\n",
    "            # Non invia dati, serve solo a capire quale interfaccia di rete usare\n",
    "            s.connect((target_ip, 80)) \n",
    "            return s.getsockname()[0]\n",
    "        except Exception:\n",
    "            return \"127.0.0.1\"\n",
    "\n",
    "CLIENT_IP = get_local_ip(MASTER_IP)\n",
    "print(f\"ðŸ“¡ Il mio IP Client (Driver) Ã¨: {CLIENT_IP}\")\n",
    "print(f\"ðŸŽ¯ Mi connetto al Master su: {MASTER_IP}\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Pulizia sessioni precedenti (Anti-Zombie)\n",
    "# -------------------------------------------------------\n",
    "try:\n",
    "    if 'spark' in globals(): spark.stop()\n",
    "    if SparkContext._active_spark_context: SparkContext._active_spark_context.stop()\n",
    "    SparkContext._active_spark_context = None\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Inizializzazione Spark Session\n",
    "# -------------------------------------------------------\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"SatStream-Distributed-Client\")\n",
    "conf.setMaster(f\"spark://{MASTER_IP}:7077\")\n",
    "\n",
    "# CONFIGURAZIONE RETE FONDAMENTALE\n",
    "# Il Driver (questo PC) deve essere raggiungibile dai Worker remoti\n",
    "conf.set(\"spark.driver.host\", CLIENT_IP)        # <--- CORRETTO: Usa l'IP del Client, non del Master\n",
    "conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\") # Ascolta su tutte le interfacce locali\n",
    "conf.set(\"spark.driver.port\", \"20000\")          # Porta fissa per firewall\n",
    "conf.set(\"spark.blockManager.port\", \"20001\")    # Porta fissa per dati\n",
    "\n",
    "# Configurazione S3/MinIO\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# Timeout e Heartbeat (utili in reti distribuite)\n",
    "conf.set(\"spark.network.timeout\", \"600s\")\n",
    "conf.set(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "\n",
    "print(\"ðŸš€ Tentativo di connessione al Cluster...\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    \n",
    "    print(f\"\\nâœ… SUCCESS! Connesso al Cluster.\")\n",
    "    print(f\"ðŸ”— Master: {spark.sparkContext.master}\")\n",
    "    print(f\" Web UI (Locale): http://{CLIENT_IP}:4040\")\n",
    "    print(f\" Web UI (Master): http://{MASTER_IP}:8080\")\n",
    "    \n",
    "    # Test veloce\n",
    "    count = spark.range(100).count()\n",
    "    print(f\"ðŸ§ª Test di calcolo distribuito: {count} righe contate.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERRORE CRITICO:\\n{e}\")\n",
    "    print(\"\\nSUGGERIMENTO: Controlla che la porta 7077 sul Master sia aperta e che 'spark.driver.host' sia raggiungibile dai Worker.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f524f",
   "metadata": {},
   "source": [
    "## Verifica dataset MinIO (boto3 + mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bd22ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lista patch via mc (conferma esistenza)\n",
      "\n",
      " mc conferma patch esistono!\n",
      " 862 patch .npz confermate via boto3!\n",
      " Prime 10:\n",
      "  2.3MB patches/task_0_worker_2.npz\n",
      "  2.4MB patches/task_1000_worker_2.npz\n",
      "  2.4MB patches/task_1001_worker_3.npz\n",
      "  2.6MB patches/task_1004_worker_9.npz\n",
      "  2.4MB patches/task_1011_worker_3.npz\n",
      "  2.4MB patches/task_1012_worker_9.npz\n",
      "  2.5MB patches/task_1016_worker_3.npz\n",
      "  2.4MB patches/task_1017_worker_2.npz\n",
      "  0.9MB patches/task_1018_worker_3.npz\n",
      "  1.7MB patches/task_101_worker_9.npz\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ BYPASS S3 Spark â†’ Usa mc + Python nativo (100% funziona)\n",
    "print(\" Lista patch via mc (conferma esistenza)\")\n",
    "result = subprocess.run([\n",
    "    \"docker\", \"exec\", \"minio\", \"mc\", \"ls\", \n",
    "    \"myminio/satellite-data/patches/\", \"|\", \"wc\", \"-l\"\n",
    "], shell=True, capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "print(\" mc conferma patch esistono!\")\n",
    "\n",
    "# ðŸ”§ S3 client Python nativo (bypassa Spark S3A)\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ADDRESS,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET_NAME, Prefix='patches/')\n",
    "files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.npz')]\n",
    "print(f\" {len(files):,} patch .npz confermate via boto3!\")\n",
    "\n",
    "# Prime 10\n",
    "print(\" Prime 10:\")\n",
    "for f in files[:10]:\n",
    "    size = next(obj['Size'] for obj in response['Contents'] if obj['Key'] == f)\n",
    "    print(f\"  {size/1024/1024:.1f}MB {f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbca1c2",
   "metadata": {},
   "source": [
    "## Estrai pixel + Feature Engineering Sentinel-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "196bc9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDINE FINALE dopo stackstac.stack() (ALFABETICO!)\n",
    "BANDS_ORDER = {\n",
    "    0: 'blue',      # B02 (490nm)\n",
    "    1: 'green',     # B03 (560nm)  \n",
    "    2: 'nir',       # B08 (842nm) â† NIR principale\n",
    "    3: 'nir08',     # B8A (865nm) â† NIR stretto\n",
    "    4: 'red',       # B04 (665nm)\n",
    "    5: 'rededge1',  # B05 (705nm)\n",
    "    6: 'rededge2',  # B06 (740nm)\n",
    "    7: 'rededge3',  # B07 (783nm)\n",
    "    8: 'swir16',    # B11 (1610nm) â† SWIR1\n",
    "    9: 'swir22'     # B12 (2190nm) â† SWIR2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de05b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… UDF fixed (np.where instead of np.divide out=)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_pixels(s3_path: str) -> List[Tuple[List[float], float]]:\n",
    "    \"\"\"\n",
    "    Estrae pixel features Sentinel-2 + labels da file .npz su MinIO S3.\n",
    "    \n",
    "    Args:\n",
    "        s3_path: Path S3 patch .npz\n",
    "\n",
    "    Output features (6 totali per pixel):\n",
    "        | Index | Feature     | Banda   | Descrizione                  |\n",
    "        |-------|-------------|---------|------------------------------|\n",
    "        | 0     | Red (B04)   | bands[3]| Riflettanza Rossa (665nm)   |\n",
    "        | 1     | NIR (B08)   | bands[7]| Infrarosso Vicino (842nm)   |\n",
    "        | 2     | SWIR1(B11)  | bands[10]| Infrarosso SWIR (1610nm)   |\n",
    "        | 3     | NDVI        | -       | (NIR-Red)/(NIR+Red)         |\n",
    "        | 4     | NDWI        | -       | (Green-SWIR)/(Green+SWIR)   |\n",
    "        | 5     | NDMI        | -       | (NIR-SWIR)/(NIR+SWIR)       |\n",
    "    \n",
    "    Returns:\n",
    "        [(features_6d, label), ...] dove features = [Red, NIR, SWIR1, NDVI, NDWI, NDMI]\n",
    "        - features_6d: [Red, NIR, SWIR1, NDVI, NDWI, NDMI]\n",
    "        - label: 0.0 (non-crop) o 1.0 (crop)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fs = s3fs.S3FileSystem(\n",
    "            key=MINIO_ACCESS_KEY, secret=MINIO_SECRET_KEY,\n",
    "            client_kwargs={'endpoint_url': MINIO_ADDRESS}\n",
    "        )\n",
    "        with fs.open(s3_path.replace('s3a://', ''), 'rb') as f:\n",
    "            data = np.load(io.BytesIO(f.read()))\n",
    "        \n",
    "        bands = data['bands']  # (10, H, W)\n",
    "        mask = data['mask'].astype(float)\n",
    "        \n",
    "        # ðŸŒ± INDICI CORRETTI stackstac alfabetico\n",
    "        b3_green = bands[1].flatten().astype(np.float32)   # 'green' â†’ index 1\n",
    "        b4_red = bands[4].flatten().astype(np.float32)     # 'red'   â†’ index 4\n",
    "        b8_nir = bands[2].flatten().astype(np.float32)     # 'nir'   â†’ index 2\n",
    "        b11_swir = bands[8].flatten().astype(np.float32)   # 'swir16'â†’ index 8\n",
    "        \n",
    "        # ðŸŒ¿ FIX: SENZA out= parameter (crea nuovo array float)\n",
    "        ndvi = np.where(\n",
    "            (b8_nir + b4_red) != 0,\n",
    "            (b8_nir - b4_red) / (b8_nir + b4_red + 1e-8),\n",
    "            0.0\n",
    "        )\n",
    "        \n",
    "        ndwi = np.where(\n",
    "            (b3_green + b11_swir) != 0,\n",
    "            (b3_green - b11_swir) / (b3_green + b11_swir + 1e-8),\n",
    "            0.0\n",
    "        )\n",
    "        \n",
    "        ndmi = np.where(\n",
    "            (b8_nir + b11_swir) != 0,\n",
    "            (b8_nir - b11_swir) / (b8_nir + b11_swir + 1e-8),\n",
    "            0.0\n",
    "        )\n",
    "        \n",
    "        # 6 features finali\n",
    "        features = np.column_stack([\n",
    "            b4_red, b8_nir, b11_swir,  # Raw bands\n",
    "            ndvi, ndwi, ndmi           # Spectral indices\n",
    "        ])\n",
    "        \n",
    "        labels = mask.flatten()\n",
    "        \n",
    "        # Filtra pixel validi (0 o 1)\n",
    "        valid = (labels == 0) | (labels == 1)\n",
    "        return [(features[valid][i].tolist(), float(labels[valid][i]))\n",
    "                for i in range(len(labels[valid]))]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error {s3_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# UDF Spark\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"features\", ArrayType(FloatType()), False),\n",
    "    StructField(\"label\", FloatType(), False)\n",
    "]))\n",
    "extract_udf = udf(extract_pixels, schema)\n",
    "\n",
    "print(\"âœ… UDF fixed (np.where instead of np.divide out=)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93807c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š bands.shape: (10, 581, 443)\n",
      "  Band 0: mean=213.9 std=123.3 range=[1, 3284]\n",
      "  Band 1: mean=396.3 std=165.3 range=[33, 3480]\n",
      "  Band 2: mean=364.9 std=199.1 range=[21, 3836]\n",
      "  Band 3: mean=2379.8 std=571.7 range=[470, 5688]\n",
      "  Band 4: mean=786.8 std=238.9 range=[149, 4039]\n",
      "  Band 5: mean=1885.6 std=426.3 range=[553, 4255]\n",
      "  Band 6: mean=2266.7 std=503.8 range=[753, 5003]\n",
      "  Band 7: mean=2571.5 std=546.3 range=[991, 5495]\n",
      "  Band 8: mean=1765.4 std=548.4 range=[523, 5081]\n",
      "  Band 9: mean=914.0 std=395.2 range=[220, 3784]\n",
      "\n",
      "ðŸŒ¿ NDVI range: [0.000, 342.560]\n",
      "   (atteso: -0.2 â†’ +0.8 per vegetazione)\n"
     ]
    }
   ],
   "source": [
    "# %% TEST: Conferma ordine bande\n",
    "first_file = files[0]\n",
    "obj = s3.get_object(Bucket='satellite-data', Key=first_file)\n",
    "data = np.load(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "print(f\"ðŸ“Š bands.shape: {data['bands'].shape}\")  # (10, 256, 256)\n",
    "\n",
    "# Stampa statistiche per identificare bande\n",
    "for i in range(10):\n",
    "    band = data['bands'][i]\n",
    "    print(f\"  Band {i}: mean={band.mean():.1f} std={band.std():.1f} \"\n",
    "          f\"range=[{band.min()}, {band.max()}]\")\n",
    "\n",
    "# Verifica NDVI\n",
    "red = data['bands'][4]\n",
    "nir = data['bands'][2]\n",
    "ndvi = (nir - red) / (nir + red + 1e-8)\n",
    "print(f\"\\nðŸŒ¿ NDVI range: [{ndvi.min():.3f}, {ndvi.max():.3f}]\")\n",
    "print(f\"   (atteso: -0.2 â†’ +0.8 per vegetazione)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f88ecb",
   "metadata": {},
   "source": [
    "## ðŸ“Š CELL 5: Estrai pixel â†’ DataFrame 6 colonne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b9e04",
   "metadata": {},
   "source": [
    "### STEP 1: Lista patch S3 (100 patch per test veloce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1643106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ 100 patch selezionate\n",
      "ðŸ”„ Esecuzione UDF (puÃ² richiedere 5-10 min)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/27 10:03:26 WARN TaskSetManager: Lost task 3.0 in stage 5.0 (TID 14) (127.0.0.1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 's3fs'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "25/12/27 10:03:26 ERROR TaskSetManager: Task 1 in stage 5.0 failed 4 times; aborting job\n",
      "25/12/27 10:03:26 WARN TaskSetManager: Lost task 7.3 in stage 5.0 (TID 37) (127.0.0.1 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 34) (127.0.0.1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 's3fs'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/27 10:03:26 WARN TaskSetManager: Lost task 5.3 in stage 5.0 (TID 38) (127.0.0.1 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 34) (127.0.0.1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 's3fs'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/27 10:03:26 WARN TaskSetManager: Lost task 10.3 in stage 5.0 (TID 36) (127.0.0.1 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 34) (127.0.0.1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 's3fs'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "[Stage 5:>                                                         (0 + 4) / 20]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 's3fs'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m pixels_raw_df \u001b[38;5;241m=\u001b[39m patches_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixels\u001b[39m\u001b[38;5;124m\"\u001b[39m, extract_udf(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m))) \\\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mcache()  \u001b[38;5;66;03m# â† CACHE risultato UDF!\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Force esecuzione UDF ADESSO (1 volta sola)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m n_patches \u001b[38;5;241m=\u001b[39m \u001b[43mpixels_raw_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_patches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m patch processate in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# STEP 4: Esplodi (veloce, usa cache)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 's3fs'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/27 10:03:30 WARN TaskSetManager: Lost task 4.0 in stage 5.0 (TID 15) (192.168.128.232 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 34) (127.0.0.1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 's3fs'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/27 10:03:30 WARN TaskSetManager: Lost task 9.0 in stage 5.0 (TID 17) (192.168.128.232 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 34) (127.0.0.1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 's3fs'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/27 10:03:30 WARN TaskSetManager: Lost task 2.0 in stage 5.0 (TID 13) (192.168.128.232 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 34) (127.0.0.1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 's3fs'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/27 10:03:30 WARN TaskSetManager: Lost task 11.1 in stage 5.0 (TID 27) (192.168.128.232 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 34) (127.0.0.1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 's3fs'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/27 10:08:24 WARN StandaloneAppClient$ClientEndpoint: Connection to 0.0.0.0:7077 failed; waiting for master to reconnect...\n",
      "25/12/27 10:08:24 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # âš¡ VELOCIZZATO: Cache + 1 solo count finale\n",
    "\n",
    "# %%\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# STEP 1-2: Paths (invariato)\n",
    "s3 = boto3.client('s3', endpoint_url=MINIO_ADDRESS, \n",
    "                  aws_access_key_id=MINIO_ACCESS_KEY, aws_secret_access_key=MINIO_SECRET_KEY)\n",
    "\n",
    "response = s3.list_objects_v2(Bucket='satellite-data', Prefix='patches/')\n",
    "files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.npz')][:100]\n",
    "paths = ['s3a://satellite-data/' + f for f in files]\n",
    "\n",
    "print(f\"ðŸŽ¯ {len(paths)} patch selezionate\")\n",
    "\n",
    "patches_df = spark.createDataFrame(\n",
    "    [(path,) for path in paths],\n",
    "    StructType([StructField(\"path\", StringType())])\n",
    ").repartition(20)  # â† Parallelizza su 20 partizioni\n",
    "\n",
    "# STEP 3: UDF + CACHE (esegui subito)\n",
    "print(\"ðŸ”„ Esecuzione UDF (puÃ² richiedere 5-10 min)...\")\n",
    "pixels_raw_df = patches_df.withColumn(\"pixels\", extract_udf(col(\"path\"))) \\\n",
    "    .cache()  # â† CACHE risultato UDF!\n",
    "\n",
    "# Force esecuzione UDF ADESSO (1 volta sola)\n",
    "n_patches = pixels_raw_df.count()\n",
    "print(f\"âœ… {n_patches} patch processate in {time.time()-start:.1f}s\")\n",
    "\n",
    "# STEP 4: Esplodi (veloce, usa cache)\n",
    "pixels_df = pixels_raw_df.select(explode(\"pixels\").alias(\"pixel\"))\n",
    "\n",
    "# STEP 5: 6 colonne + CACHE\n",
    "features_df = pixels_df.select(\n",
    "    col(\"pixel.features\")[0].alias(\"red_b4\"),\n",
    "    col(\"pixel.features\")[1].alias(\"nir_b8\"),\n",
    "    col(\"pixel.features\")[2].alias(\"swir_b11\"),\n",
    "    col(\"pixel.features\")[3].alias(\"ndvi\"),\n",
    "    col(\"pixel.features\")[4].alias(\"ndwi\"),\n",
    "    col(\"pixel.features\")[5].alias(\"ndmi\"),\n",
    "    col(\"pixel.label\").alias(\"label\")\n",
    ").cache()  # â† CACHE features finali\n",
    "\n",
    "# 1 SOLO COUNT (esegue explode + select)\n",
    "total_pixels = features_df.count()\n",
    "print(f\"âœ… {total_pixels:,} pixel in {time.time()-start:.1f}s totali\")\n",
    "\n",
    "# Statistiche (usano cache)\n",
    "features_df.groupBy(\"label\").count().orderBy(\"label\").show()\n",
    "features_df.describe(\"ndvi\", \"label\").show()\n",
    "features_df.show(5)\n",
    "\n",
    "print(f\"\\nâ±ï¸  Tempo totale: {time.time()-start:.1f}s ({(time.time()-start)/60:.1f} min)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8451b0",
   "metadata": {},
   "source": [
    "## ML Pipeline + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Train: 0 | Test: 0 pixel\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type array<float> of column features is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_df.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pixel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# TRAIN\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m train_assembled = \u001b[43massembler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m model = rf.fit(train_assembled)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Modello addestrato!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/ml/base.py:260\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/ml/util.py:270\u001b[39m, in \u001b[36mtry_remote_transform_relation.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ConnectDataFrame(plan=plan, session=session)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/ml/wrapper.py:435\u001b[39m, in \u001b[36mJavaTransformer._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: Data type array<float> of column features is not supported."
     ]
    }
   ],
   "source": [
    "# Vector Assembler (8 features â†’ vector)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"features\"], \n",
    "    outputCol=\"features_vec\",\n",
    "    handleInvalid=\"skip\" # <-- salta le righe che contengono null\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\", # La nuova colonna vettoriale\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "\n",
    "# Random Forest (bilanciato per CROP raro)\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features_vec\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=8,\n",
    "    impurity=\"gini\",\n",
    "    featureSubsetStrategy=\"sqrt\"\n",
    ")\n",
    "\n",
    "# SPLIT 80/20\n",
    "train_df, test_df = pixels_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"ðŸ“Š Train: {train_df.count():,} | Test: {test_df.count():,} pixel\")\n",
    "\n",
    "# TRAIN\n",
    "train_assembled = assembler.transform(train_df)\n",
    "model = rf.fit(train_assembled)\n",
    "\n",
    "print(\"âœ… Modello addestrato!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85992c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDIZIONI\n",
    "test_assembled = assembler.transform(test_df)\n",
    "predictions = model.transform(test_assembled)\n",
    "\n",
    "# Metriche\n",
    "roc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = roc_evaluator.evaluate(predictions)\n",
    "print(f\"ðŸŽ¯ AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "f1 = f1_evaluator.evaluate(predictions)\n",
    "print(f\"ðŸ“Š F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_df = predictions.groupBy(\"label\", \"prediction\").count().toPandas()\n",
    "print(\"ðŸ”¢ Confusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "# Feature Importance\n",
    "importances = model.featureImportances\n",
    "print(\"\\nðŸŒ¿ Feature Importance:\")\n",
    "features = [\"Red\", \"NIR\", \"SWIR1\", \"NDVI\", \"NDWI\", \"NDMI\"]\n",
    "for i, (feat, imp) in enumerate(zip(features, importances)):\n",
    "    print(f\"  {feat}: {imp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d99bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva modello su MinIO\n",
    "model.write().overwrite().save(\"s3a://satellite-data/models/crop_classifier_rf_v1\")\n",
    "print(\"âœ… Modello salvato su MinIO!\")\n",
    "\n",
    "# Test su 1 patch nuova\n",
    "test_patch = patches_df.filter(col(\"path\").contains(\"task_0\")).first()\n",
    "test_pixels = spark.createDataFrame(\n",
    "    extract_pixels(test_patch.path), \n",
    "    schema\n",
    ").select(\"features\", \"label\")\n",
    "\n",
    "test_assembled = assembler.transform(test_pixels)\n",
    "test_pred = model.transform(test_assembled)\n",
    "\n",
    "print(\"ðŸ§ª Test nuova patch:\")\n",
    "test_pred.groupBy(\"label\", \"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"âœ… Spark terminato!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
