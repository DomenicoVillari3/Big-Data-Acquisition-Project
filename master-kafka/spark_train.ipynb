{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75052fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import boto3\n",
    "from botocore.config import Config \n",
    "from typing import Tuple,List\n",
    "import sys\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import time \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb56edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_IP = \"192.168.128.236\"  # IP del Server Master\n",
    "MINIO_BUCKET_NAME=\"satellite-data\"\n",
    "MINIO_ADDRESS=\"http://192.168.128.236:9000\"\n",
    "# Configurazione MinIO\n",
    "MINIO_ENDPOINT = f\"http://{MASTER_IP}:9000\"\n",
    "MINIO_ACCESS_KEY = \"minioadmin\"\n",
    "MINIO_SECRET_KEY = \"minioadmin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4209d4",
   "metadata": {},
   "source": [
    "## Setup Spark context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09819e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ Il mio IP Client (Driver) Ã¨: 192.168.128.236\n",
      "ðŸŽ¯ Mi connetto al Master su: 192.168.128.236\n",
      "ðŸš€ Tentativo di connessione al Cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 14:52:37 WARN Utils: Your hostname, PC7 resolves to a loopback address: 127.0.1.1; using 192.168.128.236 instead (on interface eno1)\n",
      "25/12/29 14:52:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/amministratore/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/amministratore/.ivy2/cache\n",
      "The jars for the packages stored in: /home/amministratore/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f99ce4d2-ab5f-4307-99ae-5fc657c5cf94;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.7 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      ":: resolution report :: resolve 1481ms :: artifacts dl 42ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   95  |   0   |   0   |   0   ||   95  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f99ce4d2-ab5f-4307-99ae-5fc657c5cf94\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 95 already retrieved (0kB/19ms)\n",
      "25/12/29 14:52:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… SUCCESS! Connesso al Cluster.\n",
      "ðŸ”— Master: spark://192.168.128.236:7077\n",
      " Web UI (Locale): http://192.168.128.236:4040\n",
      " Web UI (Master): http://192.168.128.236:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Test di calcolo distribuito: 100 righe contate in 6.986287832260132 secondi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Trova automaticamente l'IP della TUA macchina (Client PC7)\n",
    "# che Ã¨ visibile al Master.\n",
    "def get_local_ip(target_ip):\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:\n",
    "        try:\n",
    "            # Non invia dati, serve solo a capire quale interfaccia di rete usare\n",
    "            s.connect((target_ip, 80)) \n",
    "            return s.getsockname()[0]\n",
    "        except Exception:\n",
    "            return \"127.0.0.1\"\n",
    "\n",
    "CLIENT_IP = get_local_ip(MASTER_IP)\n",
    "print(f\"ðŸ“¡ Il mio IP Client (Driver) Ã¨: {CLIENT_IP}\")\n",
    "print(f\"ðŸŽ¯ Mi connetto al Master su: {MASTER_IP}\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Pulizia sessioni precedenti (Anti-Zombie)\n",
    "# -------------------------------------------------------\n",
    "try:\n",
    "    if 'spark' in globals(): spark.stop()\n",
    "    if SparkContext._active_spark_context: SparkContext._active_spark_context.stop()\n",
    "    SparkContext._active_spark_context = None\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Inizializzazione Spark Session\n",
    "# -------------------------------------------------------\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"SatStream-Distributed-Client\")\n",
    "conf.setMaster(f\"spark://{MASTER_IP}:7077\")\n",
    "\n",
    "# CONFIGURAZIONE RETE FONDAMENTALE\n",
    "# Il Driver (questo PC) deve essere raggiungibile dai Worker remoti\n",
    "conf.set(\"spark.driver.host\", CLIENT_IP)        # <--- CORRETTO: Usa l'IP del Client, non del Master\n",
    "conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\") # Ascolta su tutte le interfacce locali\n",
    "conf.set(\"spark.driver.port\", \"20000\")          # Porta fissa per firewall\n",
    "conf.set(\"spark.blockManager.port\", \"20001\")    # Porta fissa per dati\n",
    "\n",
    "# Configurazione S3/MinIO\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "conf.set(\"spark.jars.packages\", \n",
    "         \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "         \"org.apache.hadoop:hadoop-common:3.3.4,\"\n",
    "         \"com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
    "\n",
    "# Timeout e Heartbeat (utili in reti distribuite)\n",
    "conf.set(\"spark.network.timeout\", \"600s\")\n",
    "conf.set(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "\n",
    "print(\"ðŸš€ Tentativo di connessione al Cluster...\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    \n",
    "    print(f\"\\nâœ… SUCCESS! Connesso al Cluster.\")\n",
    "    print(f\"ðŸ”— Master: {spark.sparkContext.master}\")\n",
    "    print(f\" Web UI (Locale): http://{CLIENT_IP}:4040\")\n",
    "    print(f\" Web UI (Master): http://{MASTER_IP}:8080\")\n",
    "    \n",
    "    # Test veloce\n",
    "    t=time.time()\n",
    "    count = spark.range(100).count()\n",
    "    t=time.time()-t\n",
    "    print(f\"ðŸ§ª Test di calcolo distribuito: {count} righe contate in {t} secondi.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERRORE CRITICO:\\n{e}\")\n",
    "    print(\"\\nSUGGERIMENTO: Controlla che la porta 7077 sul Master sia aperta e che 'spark.driver.host' sia raggiungibile dai Worker.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f524f",
   "metadata": {},
   "source": [
    "## Verifica dataset MinIO (boto3 + mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bd22ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lista patch via mc (conferma esistenza)\n",
      "\n",
      " mc conferma patch esistono!\n",
      " 1,000 patch .npz confermate via boto3!\n",
      " Prime 10:\n",
      "  6.0MB parquet/task_0_worker_9.parquet\n",
      "  5.6MB parquet/task_1000_worker_2.parquet\n",
      "  5.6MB parquet/task_1001_worker_9.parquet\n",
      "  5.6MB parquet/task_1002_worker_9.parquet\n",
      "  5.6MB parquet/task_1004_worker_3.parquet\n",
      "  5.6MB parquet/task_1005_worker_2.parquet\n",
      "  5.6MB parquet/task_1007_worker_9.parquet\n",
      "  5.5MB parquet/task_1009_worker_9.parquet\n",
      "  5.4MB parquet/task_100_worker_3.parquet\n",
      "  5.5MB parquet/task_1010_worker_2.parquet\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ BYPASS S3 Spark â†’ Usa mc + Python nativo (100% funziona)\n",
    "print(\" Lista patch via mc (conferma esistenza)\")\n",
    "result = subprocess.run([\n",
    "    \"docker\", \"exec\", \"minio\", \"mc\", \"ls\", \n",
    "    \"myminio/satellite-data/parquet/\", \"|\", \"wc\", \"-l\"\n",
    "], shell=True, capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "print(\" mc conferma patch esistono!\")\n",
    "\n",
    "# ðŸ”§ S3 client Python nativo (bypassa Spark S3A)\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ADDRESS,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET_NAME, Prefix='parquet/')\n",
    "files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.parquet')]\n",
    "print(f\" {len(files):,} patch .npz confermate via boto3!\")\n",
    "\n",
    "# Prime 10\n",
    "print(\" Prime 10:\")\n",
    "for f in files[:10]:\n",
    "    size = next(obj['Size'] for obj in response['Contents'] if obj['Key'] == f)\n",
    "    print(f\"  {size/1024/1024:.1f}MB {f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbca1c2",
   "metadata": {},
   "source": [
    "## Estrai pixel + Feature Engineering Sentinel-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196bc9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDINE FINALE dopo stackstac.stack() (ALFABETICO!)\n",
    "BANDS_ORDER = {\n",
    "    0: 'blue',      # B02 (490nm)\n",
    "    1: 'green',     # B03 (560nm)  \n",
    "    2: 'nir',       # B08 (842nm) â† NIR principale\n",
    "    3: 'nir08',     # B8A (865nm) â† NIR stretto\n",
    "    4: 'red',       # B04 (665nm)\n",
    "    5: 'rededge1',  # B05 (705nm)\n",
    "    6: 'rededge2',  # B06 (740nm)\n",
    "    7: 'rededge3',  # B07 (783nm)\n",
    "    8: 'swir16',    # B11 (1610nm) â† SWIR1\n",
    "    9: 'swir22'     # B12 (2190nm) â† SWIR2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93807c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Analizzo parquet/task_0_worker_9.parquet\n",
      "ðŸ“Š Shape: (257383, 13)\n",
      "ðŸ“‹ Colonne: ['patch_id', 'pixel_idx', 'label', 'band_0', 'band_1', 'band_2', 'band_3', 'band_4', 'band_5', 'band_6', 'band_7', 'band_8', 'band_9']\n",
      "\n",
      "ðŸ“ˆ Prime 5 righe:\n",
      "          patch_id  pixel_idx  label  band_0  band_1  band_2  band_3  band_4  \\\n",
      "0  task_0_worker_9          0      0    8976    8888    8904    9040    9160   \n",
      "1  task_0_worker_9          1      0    8592    8552    8496    8416    8504   \n",
      "2  task_0_worker_9          2      0    6216    5724    5508    5108    5512   \n",
      "3  task_0_worker_9          3      0    6632    6352    5684    5180    5036   \n",
      "4  task_0_worker_9          4      0    3408    3936    4492    4600    4552   \n",
      "\n",
      "   band_5  band_6  band_7  band_8  band_9  \n",
      "0    9152    8944    8744    8688    8616  \n",
      "1    8504    8384    7920    7264    6708  \n",
      "2    5824    6420    6720    6688    6612  \n",
      "3    3932    2740    2610    2708    3030  \n",
      "4    4480    4716    5156    5340    5380  \n",
      "\n",
      "ðŸŽ¯ 10 bande: ['band_0', 'band_1', 'band_2', 'band_3', 'band_4', 'band_5', 'band_6', 'band_7', 'band_8', 'band_9']\n",
      "\n",
      "ðŸ“Š STATISTICHE BANDE:\n",
      "  Band 0 (band_0): mean=2627.6 std=2226.3 range=[64, 14072]\n",
      "  Band 1 (band_1): mean=2627.2 std=2226.0 range=[64, 14084]\n",
      "  Band 2 (band_2): mean=2627.5 std=2226.4 range=[56, 14084]\n",
      "  Band 3 (band_3): mean=2627.8 std=2226.8 range=[57, 14160]\n",
      "  Band 4 (band_4): mean=2627.6 std=2226.1 range=[61, 14084]\n",
      "  Band 5 (band_5): mean=2627.6 std=2226.3 range=[64, 14084]\n",
      "  Band 6 (band_6): mean=2627.8 std=2226.9 range=[61, 14088]\n",
      "  Band 7 (band_7): mean=2627.7 std=2226.2 range=[64, 14000]\n",
      "  Band 8 (band_8): mean=2627.4 std=2226.1 range=[64, 14084]\n",
      "  Band 9 (band_9): mean=2627.5 std=2226.6 range=[58, 14088]\n",
      "\n",
      "ðŸŒ¿ TEST INDICI SPETTRALI:\n",
      "NDVI range: [0.000, 546.067] (atteso: -0.2 â†’ +0.8)\n",
      "NDWI range: [0.000, 471.417] (acque <0)\n",
      "Crop ratio: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# âœ… CORRETTO: Leggi PRIMO PARQUET\n",
    "first_file = files[0]\n",
    "print(f\"ðŸ” Analizzo {first_file}\")\n",
    "\n",
    "obj = s3.get_object(Bucket='satellite-data', Key=first_file)\n",
    "df_sample = pd.read_parquet(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "print(f\"ðŸ“Š Shape: {df_sample.shape}\")\n",
    "print(f\"ðŸ“‹ Colonne: {list(df_sample.columns)}\")\n",
    "print(\"\\nðŸ“ˆ Prime 5 righe:\")\n",
    "print(df_sample.head())\n",
    "\n",
    "# %%\n",
    "# ðŸ”¬ IDENTIFICA BANDE (stackstac order alfabetico [file:1])\n",
    "band_cols = [col for col in df_sample.columns if col.startswith('band_')]\n",
    "print(f\"\\nðŸŽ¯ {len(band_cols)} bande: {band_cols}\")\n",
    "\n",
    "# Statistiche bande\n",
    "print(\"\\nðŸ“Š STATISTICHE BANDE:\")\n",
    "for i, col in enumerate(band_cols):\n",
    "    band_stats = df_sample[col].describe()\n",
    "    print(f\"  Band {i} ({col}): mean={band_stats['mean']:.1f} std={band_stats['std']:.1f} \"\n",
    "          f\"range=[{band_stats['min']:.0f}, {band_stats['max']:.0f}]\")\n",
    "\n",
    "# %%\n",
    "# ðŸŒ¿ TEST NDVI sui parquet (Red=B04=band_4?, NIR=B08=band_2?)\n",
    "print(\"\\nðŸŒ¿ TEST INDICI SPETTRALI:\")\n",
    "\n",
    "# Ordine stackstac alfabetico da worker.py [file:1]\n",
    "# blue(0), green(1), nir(2), nir08(3), red(4), rededge1(5), rededge2(6), rededge3(7), swir16(8), swir22(9)\n",
    "red_b4 = df_sample['band_4']    # B04 Red (index 4)\n",
    "nir_b8 = df_sample['band_2']    # B08 NIR (index 2)  \n",
    "swir_b11 = df_sample['band_8']  # B11 SWIR1 (index 8)\n",
    "green_b3 = df_sample['band_1']  # B03 Green (index 1)\n",
    "\n",
    "ndvi = (nir_b8 - red_b4) / (nir_b8 + red_b4 + 1e-8)\n",
    "ndwi = (green_b3 - swir_b11) / (green_b3 + swir_b11 + 1e-8)\n",
    "\n",
    "print(f\"NDVI range: [{ndvi.min():.3f}, {ndvi.max():.3f}] (atteso: -0.2 â†’ +0.8)\")\n",
    "print(f\"NDWI range: [{ndwi.min():.3f}, {ndwi.max():.3f}] (acque <0)\")\n",
    "print(f\"Crop ratio: {df_sample['label'].mean():.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f88ecb",
   "metadata": {},
   "source": [
    "## ðŸ“Š CELL 5: Estrai pixel â†’ DataFrame 6 colonne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b9e04",
   "metadata": {},
   "source": [
    "### STEP 1: Lista patch S3 (100 patch per test veloce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1643106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 14:53:12 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset globale...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 444,875,391 | Crop: 89,192,236 (20.0%)\n",
      "âœ… Features normalizzate!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 14:53:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "ERROR:root:KeyboardInterrupt while sending command.==========>  (191 + 9) / 200]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/amministratore/miniconda3/envs/spark_env/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/amministratore/miniconda3/envs/spark_env/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/amministratore/miniconda3/envs/spark_env/lib/python3.8/socket.py\", line 681, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 13:===================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     20\u001b[0m features_df \u001b[38;5;241m=\u001b[39m features_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndvi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     21\u001b[0m     F\u001b[38;5;241m.\u001b[39mwhen((col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnir_b8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred_b4\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.001\u001b[39m,  \u001b[38;5;66;03m# â† Threshold anti-zero\u001b[39;00m\n\u001b[1;32m     22\u001b[0m            (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnir_b8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred_b4\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m/\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnir_b8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred_b4\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred_b4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnir_b8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswir_b11\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndvi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndwi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndmi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     33\u001b[0m \u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misin(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m))\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Features normalizzate!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mfeatures_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/socket.py:681\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "pixels_df = spark.read.parquet(\"s3a://satellite-data/parquet/*.parquet\")\n",
    "\n",
    "print(\"ðŸ“Š Dataset globale...\")\n",
    "total = pixels_df.count()\n",
    "crop_count = pixels_df.filter(col(\"label\") == 1).count()\n",
    "print(f\"Total: {total:,} | Crop: {crop_count:,} ({crop_count/total*100:.1f}%)\")\n",
    "\n",
    "# %%\n",
    "# âœ… NORMALIZZAZIONE + FEATURES (valori / 10000.0)\n",
    "features_df = pixels_df.select(\n",
    "    (col(\"band_4\") / 10000.0).alias(\"red_b4\"),     # B04 â†’ 0-1\n",
    "    (col(\"band_2\") / 10000.0).alias(\"nir_b8\"),     # B08 â†’ 0-1\n",
    "    (col(\"band_8\") / 10000.0).alias(\"swir_b11\"),   # B11 â†’ 0-1\n",
    "    (col(\"band_1\") / 10000.0).alias(\"green_b3\"),   # B03 â†’ 0-1\n",
    "    col(\"label\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# ðŸŒ¿ INDICI SPETTRALI (sicuri)\n",
    "features_df = features_df.withColumn(\"ndvi\", \n",
    "    F.when((col(\"nir_b8\") + col(\"red_b4\")) > 0.001,  # â† Threshold anti-zero\n",
    "           (col(\"nir_b8\") - col(\"red_b4\")) / (col(\"nir_b8\") + col(\"red_b4\")))\n",
    "    .otherwise(0.0)) \\\n",
    ".withColumn(\"ndwi\", \n",
    "    F.when((col(\"green_b3\") + col(\"swir_b11\")) > 0.001,\n",
    "           (col(\"green_b3\") - col(\"swir_b11\")) / (col(\"green_b3\") + col(\"swir_b11\")))\n",
    "    .otherwise(0.0)) \\\n",
    ".withColumn(\"ndmi\", \n",
    "    F.when((col(\"nir_b8\") + col(\"swir_b11\")) > 0.001,\n",
    "           (col(\"nir_b8\") - col(\"swir_b11\")) / (col(\"nir_b8\") + col(\"swir_b11\")))\n",
    "    .otherwise(0.0)) \\\n",
    ".select(\"red_b4\", \"nir_b8\", \"swir_b11\", \"ndvi\", \"ndwi\", \"ndmi\", \"label\") \\\n",
    ".filter(col(\"label\").isin(0.0, 1.0)).repartition(200)\n",
    "\n",
    "print(\"âœ… Features normalizzate!\")\n",
    "features_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pixels_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8451b0",
   "metadata": {},
   "source": [
    "## ML Pipeline + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, rand\n",
    "\n",
    "# ðŸŽ¯ BILANCIA COMPLETO: Usa TUTTI i CROP + Sotto-campiona NON-CROP\n",
    "print(\"Forzo bilanciamento 50/50 usando TUTTI i crop...\")\n",
    "crop_df = features_df.filter(col(\"label\") == 1.0)   # CROP (usa TUTTI!)\n",
    "noncrop_df = features_df.filter(col(\"label\") == 0.0) # NON-CROP\n",
    "\n",
    "print(f\"ðŸ“Š Originale: Crop={crop_df.count():,}, NonCrop={noncrop_df.count():,}\")\n",
    "\n",
    "# ðŸ”„ Calcola quante NON-CROP prendere per matchare CROP\n",
    "n_crop = crop_df.count()\n",
    "n_noncrop_needed = n_crop  # Stesso numero di noncrop per 50/50\n",
    "\n",
    "# SPLIT 80/20 per ENTRAMBI\n",
    "train_crop, test_crop = crop_df.randomSplit([0.8, 0.2], seed=42)\n",
    "train_noncrop = noncrop_df.sample(False, 0.8 * n_noncrop_needed / noncrop_df.count(), seed=42)\n",
    "test_noncrop = noncrop_df.sample(False, 0.2 * n_noncrop_needed / noncrop_df.count(), seed=42)\n",
    "\n",
    "train_df = train_crop.union(train_noncrop)\n",
    "test_df = test_crop.union(test_noncrop)\n",
    "\n",
    "print(f\"âœ… Train 50/50: Crop={train_df.filter(col('label')==1).count():,}, NonCrop={train_df.filter(col('label')==0).count():,}\")\n",
    "print(f\"âœ… Test 50/50: Crop={test_df.filter(col('label')==1).count():,}, NonCrop={test_df.filter(col('label')==0).count():,}\")\n",
    "print(f\"ðŸ“Š % usato: Crop=100%, NonCrop={((train_noncrop.count() + test_noncrop.count()) / noncrop_df.count() * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Train: 0 | Test: 0 pixel\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type array<float> of column features is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_df.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pixel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# TRAIN\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m train_assembled = \u001b[43massembler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m model = rf.fit(train_assembled)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Modello addestrato!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/ml/base.py:260\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/ml/util.py:270\u001b[39m, in \u001b[36mtry_remote_transform_relation.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ConnectDataFrame(plan=plan, session=session)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/ml/wrapper.py:435\u001b[39m, in \u001b[36mJavaTransformer._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: Data type array<float> of column features is not supported."
     ]
    }
   ],
   "source": [
    "# Pipeline invariata (giÃ  perfetta)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"red_b4\", \"nir_b8\", \"swir_b11\", \"ndvi\", \"ndwi\", \"ndmi\"], \n",
    "    outputCol=\"features_vec\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", \n",
    "                        outputCol=\"scaled_features\", \n",
    "                        withMean=True, \n",
    "                        withStd=True)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_features\", labelCol=\"label\",\n",
    "    numTrees=100, maxDepth=10, impurity=\"gini\",\n",
    "    featureSubsetStrategy=\"sqrt\", seed=42\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# ðŸš€ TRAIN UNA VOLTA (bilanciato al 100%!)\n",
    "model = pipeline.fit(train_df)\n",
    "print(\"âœ… Modello addestrato 50/50!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85992c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š TEST\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Metriche\n",
    "auc = BinaryClassificationEvaluator(labelCol=\"label\").evaluate(predictions)\n",
    "f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(predictions)\n",
    "\n",
    "print(f\"ðŸŽ¯ AUC-ROC: {auc:.4f}\")\n",
    "print(f\"ðŸ“ˆ F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "# ðŸŒ¿ Feature Importance\n",
    "importances = model.stages[-1].featureImportances\n",
    "features = [\"Red(B4)\", \"NIR(B8)\", \"SWIR(B11)\", \"NDVI\", \"NDWI\", \"NDMI\"]\n",
    "for i, (feat, imp) in enumerate(zip(features, importances)):\n",
    "    print(f\"ðŸŒ¿ {feat}: {imp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d99bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"s3a://satellite-data/models/crop_rf_v1\")\n",
    "print(\"âœ… SALVATO su MinIO!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"âœ… Spark terminato!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
