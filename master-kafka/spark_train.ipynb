{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75052fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import io\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import boto3\n",
    "from botocore.config import Config \n",
    "from typing import Tuple,List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb56edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_BUCKET_NAME=\"satellite-data\"\n",
    "MINIO_ADDRESS=\"http://192.168.128.236:9000\"\n",
    "MINIO_ID=\"minioadmin\"\n",
    "MINIO_KEY=\"minioadmin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4209d4",
   "metadata": {},
   "source": [
    "## Setup Spark context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c3eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SparkContext pulito!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from pyspark import SparkContext\n",
    "SparkContext._active_spark_context = None\n",
    "print(\" SparkContext pulito!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c87eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/26 11:24:50 WARN Utils: Your hostname, PC7, resolves to a loopback address: 127.0.1.1; using 192.168.128.236 instead (on interface eno1)\n",
      "25/12/26 11:24:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/26 11:24:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark LOCAL pronto! CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crop-Classifier\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ADDRESS) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark LOCAL pronto! CPUs: {spark.sparkContext.defaultParallelism}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f524f",
   "metadata": {},
   "source": [
    "## Verifica dataset MinIO (boto3 + mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bd22ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lista patch via mc (conferma esistenza)\n",
      "\n",
      " mc conferma patch esistono!\n",
      " 862 patch .npz confermate via boto3!\n",
      " Prime 10:\n",
      "  2.3MB patches/task_0_worker_2.npz\n",
      "  2.4MB patches/task_1000_worker_2.npz\n",
      "  2.4MB patches/task_1001_worker_3.npz\n",
      "  2.6MB patches/task_1004_worker_9.npz\n",
      "  2.4MB patches/task_1011_worker_3.npz\n",
      "  2.4MB patches/task_1012_worker_9.npz\n",
      "  2.5MB patches/task_1016_worker_3.npz\n",
      "  2.4MB patches/task_1017_worker_2.npz\n",
      "  0.9MB patches/task_1018_worker_3.npz\n",
      "  1.7MB patches/task_101_worker_9.npz\n"
     ]
    }
   ],
   "source": [
    "# üîß BYPASS S3 Spark ‚Üí Usa mc + Python nativo (100% funziona)\n",
    "print(\" Lista patch via mc (conferma esistenza)\")\n",
    "result = subprocess.run([\n",
    "    \"docker\", \"exec\", \"minio\", \"mc\", \"ls\", \n",
    "    \"myminio/satellite-data/patches/\", \"|\", \"wc\", \"-l\"\n",
    "], shell=True, capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "print(\" mc conferma patch esistono!\")\n",
    "\n",
    "# üîß S3 client Python nativo (bypassa Spark S3A)\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ADDRESS,\n",
    "    aws_access_key_id=MINIO_ID,\n",
    "    aws_secret_access_key=MINIO_KEY\n",
    ")\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET_NAME, Prefix='patches/')\n",
    "files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.npz')]\n",
    "print(f\" {len(files):,} patch .npz confermate via boto3!\")\n",
    "\n",
    "# Prime 10\n",
    "print(\" Prime 10:\")\n",
    "for f in files[:10]:\n",
    "    size = next(obj['Size'] for obj in response['Contents'] if obj['Key'] == f)\n",
    "    print(f\"  {size/1024/1024:.1f}MB {f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbca1c2",
   "metadata": {},
   "source": [
    "## Estrai pixel + Feature Engineering Sentinel-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "196bc9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDINE FINALE dopo stackstac.stack() (ALFABETICO!)\n",
    "BANDS_ORDER = {\n",
    "    0: 'blue',      # B02 (490nm)\n",
    "    1: 'green',     # B03 (560nm)  \n",
    "    2: 'nir',       # B08 (842nm) ‚Üê NIR principale\n",
    "    3: 'nir08',     # B8A (865nm) ‚Üê NIR stretto\n",
    "    4: 'red',       # B04 (665nm)\n",
    "    5: 'rededge1',  # B05 (705nm)\n",
    "    6: 'rededge2',  # B06 (740nm)\n",
    "    7: 'rededge3',  # B07 (783nm)\n",
    "    8: 'swir16',    # B11 (1610nm) ‚Üê SWIR1\n",
    "    9: 'swir22'     # B12 (2190nm) ‚Üê SWIR2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de05b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ UDF fixed (np.where instead of np.divide out=)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amministratore/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/sql/udf.py:134: UserWarning: Cannot infer the eval type from type hints. \n",
      "  warnings.warn(\"Cannot infer the eval type from type hints. \", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_pixels(s3_path: str) -> List[Tuple[List[float], float]]:\n",
    "    \"\"\"\n",
    "    Estrae pixel features Sentinel-2 + labels da file .npz su MinIO S3.\n",
    "    \n",
    "    Args:\n",
    "        s3_path: Path S3 patch .npz\n",
    "\n",
    "    Output features (6 totali per pixel):\n",
    "        | Index | Feature     | Banda   | Descrizione                  |\n",
    "        |-------|-------------|---------|------------------------------|\n",
    "        | 0     | Red (B04)   | bands[3]| Riflettanza Rossa (665nm)   |\n",
    "        | 1     | NIR (B08)   | bands[7]| Infrarosso Vicino (842nm)   |\n",
    "        | 2     | SWIR1(B11)  | bands[10]| Infrarosso SWIR (1610nm)   |\n",
    "        | 3     | NDVI        | -       | (NIR-Red)/(NIR+Red)         |\n",
    "        | 4     | NDWI        | -       | (Green-SWIR)/(Green+SWIR)   |\n",
    "        | 5     | NDMI        | -       | (NIR-SWIR)/(NIR+SWIR)       |\n",
    "    \n",
    "    Returns:\n",
    "        [(features_6d, label), ...] dove features = [Red, NIR, SWIR1, NDVI, NDWI, NDMI]\n",
    "        - features_6d: [Red, NIR, SWIR1, NDVI, NDWI, NDMI]\n",
    "        - label: 0.0 (non-crop) o 1.0 (crop)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fs = s3fs.S3FileSystem(\n",
    "            key=MINIO_ID, secret=MINIO_KEY,\n",
    "            client_kwargs={'endpoint_url': MINIO_ADDRESS}\n",
    "        )\n",
    "        with fs.open(s3_path.replace('s3a://', ''), 'rb') as f:\n",
    "            data = np.load(io.BytesIO(f.read()))\n",
    "        \n",
    "        bands = data['bands']  # (10, H, W)\n",
    "        mask = data['mask'].astype(float)\n",
    "        \n",
    "        # üå± INDICI CORRETTI stackstac alfabetico\n",
    "        b3_green = bands[1].flatten().astype(np.float32)   # 'green' ‚Üí index 1\n",
    "        b4_red = bands[4].flatten().astype(np.float32)     # 'red'   ‚Üí index 4\n",
    "        b8_nir = bands[2].flatten().astype(np.float32)     # 'nir'   ‚Üí index 2\n",
    "        b11_swir = bands[8].flatten().astype(np.float32)   # 'swir16'‚Üí index 8\n",
    "        \n",
    "        # üåø FIX: SENZA out= parameter (crea nuovo array float)\n",
    "        ndvi = np.where(\n",
    "            (b8_nir + b4_red) != 0,\n",
    "            (b8_nir - b4_red) / (b8_nir + b4_red + 1e-8),\n",
    "            0.0\n",
    "        )\n",
    "        \n",
    "        ndwi = np.where(\n",
    "            (b3_green + b11_swir) != 0,\n",
    "            (b3_green - b11_swir) / (b3_green + b11_swir + 1e-8),\n",
    "            0.0\n",
    "        )\n",
    "        \n",
    "        ndmi = np.where(\n",
    "            (b8_nir + b11_swir) != 0,\n",
    "            (b8_nir - b11_swir) / (b8_nir + b11_swir + 1e-8),\n",
    "            0.0\n",
    "        )\n",
    "        \n",
    "        # 6 features finali\n",
    "        features = np.column_stack([\n",
    "            b4_red, b8_nir, b11_swir,  # Raw bands\n",
    "            ndvi, ndwi, ndmi           # Spectral indices\n",
    "        ])\n",
    "        \n",
    "        labels = mask.flatten()\n",
    "        \n",
    "        # Filtra pixel validi (0 o 1)\n",
    "        valid = (labels == 0) | (labels == 1)\n",
    "        return [(features[valid][i].tolist(), float(labels[valid][i]))\n",
    "                for i in range(len(labels[valid]))]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error {s3_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# UDF Spark\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"features\", ArrayType(FloatType()), False),\n",
    "    StructField(\"label\", FloatType(), False)\n",
    "]))\n",
    "extract_udf = udf(extract_pixels, schema)\n",
    "\n",
    "print(\"‚úÖ UDF fixed (np.where instead of np.divide out=)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93807c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä bands.shape: (10, 581, 443)\n",
      "  Band 0: mean=213.9 std=123.3 range=[1, 3284]\n",
      "  Band 1: mean=396.3 std=165.3 range=[33, 3480]\n",
      "  Band 2: mean=364.9 std=199.1 range=[21, 3836]\n",
      "  Band 3: mean=2379.8 std=571.7 range=[470, 5688]\n",
      "  Band 4: mean=786.8 std=238.9 range=[149, 4039]\n",
      "  Band 5: mean=1885.6 std=426.3 range=[553, 4255]\n",
      "  Band 6: mean=2266.7 std=503.8 range=[753, 5003]\n",
      "  Band 7: mean=2571.5 std=546.3 range=[991, 5495]\n",
      "  Band 8: mean=1765.4 std=548.4 range=[523, 5081]\n",
      "  Band 9: mean=914.0 std=395.2 range=[220, 3784]\n",
      "\n",
      "üåø NDVI range: [0.000, 342.560]\n",
      "   (atteso: -0.2 ‚Üí +0.8 per vegetazione)\n"
     ]
    }
   ],
   "source": [
    "# %% TEST: Conferma ordine bande\n",
    "first_file = files[0]\n",
    "obj = s3.get_object(Bucket='satellite-data', Key=first_file)\n",
    "data = np.load(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "print(f\"üìä bands.shape: {data['bands'].shape}\")  # (10, 256, 256)\n",
    "\n",
    "# Stampa statistiche per identificare bande\n",
    "for i in range(10):\n",
    "    band = data['bands'][i]\n",
    "    print(f\"  Band {i}: mean={band.mean():.1f} std={band.std():.1f} \"\n",
    "          f\"range=[{band.min()}, {band.max()}]\")\n",
    "\n",
    "# Verifica NDVI\n",
    "red = data['bands'][4]\n",
    "nir = data['bands'][2]\n",
    "ndvi = (nir - red) / (nir + red + 1e-8)\n",
    "print(f\"\\nüåø NDVI range: [{ndvi.min():.3f}, {ndvi.max():.3f}]\")\n",
    "print(f\"   (atteso: -0.2 ‚Üí +0.8 per vegetazione)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f88ecb",
   "metadata": {},
   "source": [
    "## üìä CELL 5: Estrai pixel ‚Üí DataFrame 6 colonne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b9e04",
   "metadata": {},
   "source": [
    "### STEP 1: Lista patch S3 (100 patch per test veloce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ 100 patch selezionate\n",
      "üîÑ Esecuzione UDF (pu√≤ richiedere 5-10 min)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                (0 + 8) / 20]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/amministratore/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amministratore/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_0 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_6 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_6 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_1 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_3 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_5 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_5 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_4 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_4 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_2 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/12/26 11:39:48 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 8)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/26 11:39:48 ERROR Executor: Exception in task 5.0 in stage 2.0 (TID 13)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_2 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 ERROR Executor: Exception in task 3.0 in stage 2.0 (TID 11)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/26 11:39:48 ERROR Executor: Exception in task 4.0 in stage 2.0 (TID 12)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/26 11:39:48 ERROR Executor: Exception in task 6.0 in stage 2.0 (TID 14)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_7 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/12/26 11:39:48 ERROR Executor: Exception in task 1.0 in stage 2.0 (TID 9)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_7 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 10)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/26 11:39:48 ERROR Executor: Exception in task 7.0 in stage 2.0 (TID 15)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/26 11:39:48 WARN TaskSetManager: Lost task 6.0 in stage 2.0 (TID 14) (192.168.128.236 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/12/26 11:39:48 ERROR TaskSetManager: Task 6 in stage 2.0 failed 1 times; aborting job\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_9 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 WARN BlockManager: Putting block rdd_13_10 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/26 11:39:48 WARN BlockManager: Block rdd_13_10 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:48 WARN TaskSetManager: Lost task 10.0 in stage 2.0 (TID 18) (192.168.128.236 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 14) (192.168.128.236 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/26 11:39:48 WARN TaskSetManager: Lost task 9.0 in stage 2.0 (TID 17) (192.168.128.236 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 14) (192.168.128.236 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "[Stage 2:>                                                         (0 + 1) / 20]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m pixels_raw_df = patches_df.withColumn(\u001b[33m\"\u001b[39m\u001b[33mpixels\u001b[39m\u001b[33m\"\u001b[39m, extract_udf(col(\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m))) \\\n\u001b[32m     26\u001b[39m     .cache()  \u001b[38;5;66;03m# ‚Üê CACHE risultato UDF!\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Force esecuzione UDF ADESSO (1 volta sola)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m n_patches = \u001b[43mpixels_raw_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_patches\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m patch processate in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# STEP 4: Esplodi (veloce, usa cache)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/py4j/clientserver.py:535\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    536\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    537\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    538\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/amministratore/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 233, in manager\n",
      "    code = worker(sock, authenticated)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amministratore/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 82, in worker\n",
      "    worker_main(infile, outfile)\n",
      "  File \"/home/amministratore/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 974, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amministratore/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 3405, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "       ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amministratore/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/26 11:39:52 WARN PythonUDFWithNamedArgumentsRunner: Incomplete task 8.0 in stage 2 (TID 16) interrupted: Attempting to kill Python Worker\n",
      "25/12/26 11:39:52 WARN BlockManager: Putting block rdd_13_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/26 11:39:52 WARN BlockManager: Block rdd_13_8 could not be removed as it was not found on disk or in memory\n",
      "25/12/26 11:39:52 WARN TaskSetManager: Lost task 8.0 in stage 2.0 (TID 16) (192.168.128.236 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 14) (192.168.128.236 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1388271/4216968101.py\", line 72, in extract_pixels\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:169)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:340)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1659)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # ‚ö° VELOCIZZATO: Cache + 1 solo count finale\n",
    "\n",
    "# %%\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# STEP 1-2: Paths (invariato)\n",
    "s3 = boto3.client('s3', endpoint_url=MINIO_ADDRESS, \n",
    "                  aws_access_key_id=MINIO_ID, aws_secret_access_key=MINIO_KEY)\n",
    "\n",
    "response = s3.list_objects_v2(Bucket='satellite-data', Prefix='patches/')\n",
    "files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.npz')][:100]\n",
    "paths = ['s3a://satellite-data/' + f for f in files]\n",
    "\n",
    "print(f\"üéØ {len(paths)} patch selezionate\")\n",
    "\n",
    "patches_df = spark.createDataFrame(\n",
    "    [(path,) for path in paths],\n",
    "    StructType([StructField(\"path\", StringType())])\n",
    ").repartition(20)  # ‚Üê Parallelizza su 20 partizioni\n",
    "\n",
    "# STEP 3: UDF + CACHE (esegui subito)\n",
    "print(\"üîÑ Esecuzione UDF (pu√≤ richiedere 5-10 min)...\")\n",
    "pixels_raw_df = patches_df.withColumn(\"pixels\", extract_udf(col(\"path\"))) \\\n",
    "    .cache()  # ‚Üê CACHE risultato UDF!\n",
    "\n",
    "# Force esecuzione UDF ADESSO (1 volta sola)\n",
    "n_patches = pixels_raw_df.count()\n",
    "print(f\"‚úÖ {n_patches} patch processate in {time.time()-start:.1f}s\")\n",
    "\n",
    "# STEP 4: Esplodi (veloce, usa cache)\n",
    "pixels_df = pixels_raw_df.select(explode(\"pixels\").alias(\"pixel\"))\n",
    "\n",
    "# STEP 5: 6 colonne + CACHE\n",
    "features_df = pixels_df.select(\n",
    "    col(\"pixel.features\")[0].alias(\"red_b4\"),\n",
    "    col(\"pixel.features\")[1].alias(\"nir_b8\"),\n",
    "    col(\"pixel.features\")[2].alias(\"swir_b11\"),\n",
    "    col(\"pixel.features\")[3].alias(\"ndvi\"),\n",
    "    col(\"pixel.features\")[4].alias(\"ndwi\"),\n",
    "    col(\"pixel.features\")[5].alias(\"ndmi\"),\n",
    "    col(\"pixel.label\").alias(\"label\")\n",
    ").cache()  # ‚Üê CACHE features finali\n",
    "\n",
    "# 1 SOLO COUNT (esegue explode + select)\n",
    "total_pixels = features_df.count()\n",
    "print(f\"‚úÖ {total_pixels:,} pixel in {time.time()-start:.1f}s totali\")\n",
    "\n",
    "# Statistiche (usano cache)\n",
    "features_df.groupBy(\"label\").count().orderBy(\"label\").show()\n",
    "features_df.describe(\"ndvi\", \"label\").show()\n",
    "features_df.show(5)\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Tempo totale: {time.time()-start:.1f}s ({(time.time()-start)/60:.1f} min)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8451b0",
   "metadata": {},
   "source": [
    "## ML Pipeline + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train: 0 | Test: 0 pixel\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type array<float> of column features is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_df.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pixel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# TRAIN\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m train_assembled = \u001b[43massembler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m model = rf.fit(train_assembled)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Modello addestrato!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/ml/base.py:260\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/ml/util.py:270\u001b[39m, in \u001b[36mtry_remote_transform_relation.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ConnectDataFrame(plan=plan, session=session)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/ml/wrapper.py:435\u001b[39m, in \u001b[36mJavaTransformer._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Big Data Acquisition/progetto mimmo/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: Data type array<float> of column features is not supported."
     ]
    }
   ],
   "source": [
    "# Vector Assembler (8 features ‚Üí vector)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"features\"], \n",
    "    outputCol=\"features_vec\",\n",
    "    handleInvalid=\"skip\" # <-- salta le righe che contengono null\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\", # La nuova colonna vettoriale\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "\n",
    "# Random Forest (bilanciato per CROP raro)\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features_vec\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=8,\n",
    "    impurity=\"gini\",\n",
    "    featureSubsetStrategy=\"sqrt\"\n",
    ")\n",
    "\n",
    "# SPLIT 80/20\n",
    "train_df, test_df = pixels_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"üìä Train: {train_df.count():,} | Test: {test_df.count():,} pixel\")\n",
    "\n",
    "# TRAIN\n",
    "train_assembled = assembler.transform(train_df)\n",
    "model = rf.fit(train_assembled)\n",
    "\n",
    "print(\"‚úÖ Modello addestrato!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85992c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDIZIONI\n",
    "test_assembled = assembler.transform(test_df)\n",
    "predictions = model.transform(test_assembled)\n",
    "\n",
    "# Metriche\n",
    "roc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = roc_evaluator.evaluate(predictions)\n",
    "print(f\"üéØ AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "f1 = f1_evaluator.evaluate(predictions)\n",
    "print(f\"üìä F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_df = predictions.groupBy(\"label\", \"prediction\").count().toPandas()\n",
    "print(\"üî¢ Confusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "# Feature Importance\n",
    "importances = model.featureImportances\n",
    "print(\"\\nüåø Feature Importance:\")\n",
    "features = [\"Red\", \"NIR\", \"SWIR1\", \"NDVI\", \"NDWI\", \"NDMI\"]\n",
    "for i, (feat, imp) in enumerate(zip(features, importances)):\n",
    "    print(f\"  {feat}: {imp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d99bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva modello su MinIO\n",
    "model.write().overwrite().save(\"s3a://satellite-data/models/crop_classifier_rf_v1\")\n",
    "print(\"‚úÖ Modello salvato su MinIO!\")\n",
    "\n",
    "# Test su 1 patch nuova\n",
    "test_patch = patches_df.filter(col(\"path\").contains(\"task_0\")).first()\n",
    "test_pixels = spark.createDataFrame(\n",
    "    extract_pixels(test_patch.path), \n",
    "    schema\n",
    ").select(\"features\", \"label\")\n",
    "\n",
    "test_assembled = assembler.transform(test_pixels)\n",
    "test_pred = model.transform(test_assembled)\n",
    "\n",
    "print(\"üß™ Test nuova patch:\")\n",
    "test_pred.groupBy(\"label\", \"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"‚úÖ Spark terminato!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
